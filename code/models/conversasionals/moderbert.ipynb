{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Invalid device string: '/job:localhost/replica:0/task:0/device:CPU:0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m pooler_outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_transcripts\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m---> 30\u001b[0m     last_hidden_state, pooler_output \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_and_get_bert_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     last_hidden_states\u001b[38;5;241m.\u001b[39mappend(last_hidden_state\u001b[38;5;241m.\u001b[39mnumpy())  \n\u001b[0;32m     32\u001b[0m     pooler_outputs\u001b[38;5;241m.\u001b[39mappend(pooler_output\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "Cell \u001b[1;32mIn[3], line 23\u001b[0m, in \u001b[0;36mpreprocess_and_get_bert_embeddings\u001b[1;34m(text, tokenizer, model, max_length)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_and_get_bert_embeddings\u001b[39m(text, tokenizer, model, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m):\n\u001b[0;32m     15\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[0;32m     16\u001b[0m         text,\n\u001b[0;32m     17\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length\n\u001b[0;32m     21\u001b[0m     )\n\u001b[1;32m---> 23\u001b[0m     bert_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bert_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state, bert_outputs\u001b[38;5;241m.\u001b[39mpooler_output\n",
      "File \u001b[1;32mc:\\Users\\lclai\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lclai\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lclai\\miniconda3\\Lib\\site-packages\\transformers\\models\\modernbert\\modeling_modernbert.py:952\u001b[0m, in \u001b[0;36mModernBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, sliding_window_mask, position_ids, inputs_embeds, indices, cu_seqlens, max_seqlen, batch_size, seq_len, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    951\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 952\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    954\u001b[0m     attention_mask, sliding_window_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_attention_mask(\n\u001b[0;32m    955\u001b[0m         attention_mask, output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions\n\u001b[0;32m    956\u001b[0m     )\n\u001b[0;32m    958\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids\u001b[38;5;241m=\u001b[39minput_ids, inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Invalid device string: '/job:localhost/replica:0/task:0/device:CPU:0'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "MODEL_NAME = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert_model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Cargar datos\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\lclai\\\\Desktop\\\\data_dementia\\\\clean\\\\pitt_withoutwords.csv\")\n",
    "data = data[[\"label\", \"clean_transcripts\"]]\n",
    "data = data[(data['label'] == 0) | (data['label'] == 1)]\n",
    "\n",
    "def preprocess_and_get_bert_embeddings(text, tokenizer, model, max_length=512):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"tf\",\n",
    "        padding='max_length',  \n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    bert_outputs = model(**inputs)\n",
    "    return bert_outputs.last_hidden_state, bert_outputs.pooler_output \n",
    "\n",
    "last_hidden_states = []\n",
    "pooler_outputs = []\n",
    "\n",
    "for text in data['clean_transcripts']:\n",
    "    last_hidden_state, pooler_output = preprocess_and_get_bert_embeddings(text, tokenizer, bert_model)\n",
    "    last_hidden_states.append(last_hidden_state.numpy())  \n",
    "    pooler_outputs.append(pooler_output.numpy())\n",
    "\n",
    "last_hidden_states_tensor = tf.convert_to_tensor(last_hidden_states, dtype=tf.float32)\n",
    "pooler_outputs_tensor = tf.convert_to_tensor(pooler_outputs, dtype=tf.float32)\n",
    "\n",
    "print(f\"Last Hidden States Tensor Shape: {last_hidden_states_tensor.shape}\")\n",
    "print(f\"Pooler Outputs Tensor Shape: {pooler_outputs_tensor.shape}\")\n",
    "\n",
    "# Cargar datos adicionales\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\lclai\\\\Desktop\\\\data_dementia\\\\clean\\\\pitt_withoutwords.csv\")\n",
    "df = df[(df['label'] == 0) | (df['label'] == 1)]\n",
    "df = df[[\"sex\",\"age\",\"label\"]]\n",
    "df[\"sex\"] = df[\"sex\"].map({\"M\": 0, \"F\": 1})  \n",
    "X_numeric = df[[\"age\", \"sex\"]].values  \n",
    "\n",
    "X_bert = last_hidden_states_tensor[:, 0, :].numpy()\n",
    "bert_pooled = np.mean(X_bert, axis=1)  # Reduce a (496, 768)\n",
    "X_train = np.hstack((X_numeric, bert_pooled))\n",
    "\n",
    "y = df['label']\n",
    "\n",
    "# Modelos\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200], \n",
    "        'max_depth': [None, 10, 20], \n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],  \n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 100, 200],  \n",
    "        'learning_rate': [0.01, 0.1, 0.2], \n",
    "        'max_depth': [3, 6, 10],\n",
    "        'subsample': [0.8, 1.0], \n",
    "    }\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name in models:\n",
    "    model = models[model_name]\n",
    "    param_grid = param_grids[model_name]\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "    \n",
    "    grid_search.fit(X_train, y)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    auc = grid_search.best_score_\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'mean_auc': auc\n",
    "    }\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Best Params: {result['best_params']}\")\n",
    "    print(f\"Mean AUC: {result['mean_auc']:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Establecer el dispositivo a CPU explÃ­citamente\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Cargar el modelo y moverlo al dispositivo correcto\n",
    "bert_model = bert_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U transformers>=4.48.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.6.0-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\lclai\\miniconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\lclai\\miniconda3\\lib\\site-packages (from torch) (4.10.0)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lclai\\miniconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lclai\\miniconda3\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lclai\\miniconda3\\lib\\site-packages (from jinja2->torch) (3.0.1)\n",
      "Using cached torch-2.6.0-cp311-cp311-win_amd64.whl (204.2 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, torch\n",
      "Successfully installed mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.18.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow)\n",
      "  Using cached tensorflow_intel-2.18.0-cp311-cp311-win_amd64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lclai\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lclai\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\lclai\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\lclai\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lclai\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lclai\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\lclai\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.10.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached wrapt-1.17.2-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached grpcio-1.70.0-cp311-cp311-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\lclai\\miniconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Collecting h5py>=3.11.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading h5py-3.13.0-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached ml_dtypes-0.4.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lclai\\miniconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.41.2)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached optree-0.14.0-cp311-cp311-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lclai\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lclai\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lclai\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lclai\\miniconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\lclai\\miniconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.1)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lclai\\miniconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached tensorflow-2.18.0-cp311-cp311-win_amd64.whl (7.5 kB)\n",
      "Using cached tensorflow_intel-2.18.0-cp311-cp311-win_amd64.whl (390.2 MB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.70.0-cp311-cp311-win_amd64.whl (4.3 MB)\n",
      "Downloading h5py-3.13.0-cp311-cp311-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.0/3.0 MB 29.1 MB/s eta 0:00:00\n",
      "Using cached keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Using cached ml_dtypes-0.4.1-cp311-cp311-win_amd64.whl (126 kB)\n",
      "Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "Using cached termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached wrapt-1.17.2-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Using cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Using cached optree-0.14.0-cp311-cp311-win_amd64.whl (300 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, ml-dtypes, mdurl, markdown, h5py, grpcio, google-pasta, gast, astunparse, tensorboard, markdown-it-py, rich, keras, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.3.2\n",
      "    Uninstalling ml-dtypes-0.3.2:\n",
      "      Successfully uninstalled ml-dtypes-0.3.2\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.70.0 h5py-3.13.0 keras-3.8.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 optree-0.14.0 protobuf-5.29.3 rich-13.9.4 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-intel-2.18.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.5.0 werkzeug-3.1.3 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
